[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Articles",
    "section": "",
    "text": "ExLlamaV2: The Fastest Library to Run LLMs\n\n\nQuantize and run EXL2 models\n\n\n\nLarge Language Models\n\n\n\n\n\n\nNov 19, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantize Llama models with GGUF and llama.cpp\n\n\nGGML vs. GPTQ vs. NF4\n\n\n\nLarge Language Models\n\n\n\n\n\n\nSep 3, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Beginner’s Guide to LLM Fine-Tuning\n\n\nHow to fine-tune Llama and other LLMs with one tool\n\n\n\nLarge Language Models\n\n\n\n\n\n\nAug 27, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4-bit LLM Quantization with GPTQ\n\n\nQuantize your own open-source LLMs to run them on consumer hardware\n\n\n\nLarge Language Models\n\n\n\n\n\n\nJul 30, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-Tune Your Own Llama 2 Model in a Colab Notebook\n\n\nA practical introduction to LLM fine-tuning\n\n\n\nLarge Language Models\n\n\n\n\n\n\nJul 24, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Weight Quantization\n\n\nLarge language model optimization using 8-bit quantization\n\n\n\nLarge Language Models\n\n\n\n\n\n\nJul 6, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html",
    "title": "Quantize Llama models with GGUF and llama.cpp",
    "section": "",
    "text": "Due to the massive size of Large Language Models (LLMs), quantization has become an essential technique to run them efficiently. By reducing the precision of their weights, you can save memory and speed up inference while preserving most of the model’s performance. Recently, 8-bit and 4-bit quantization unlocked the possibility of running LLMs on consumer hardware. Coupled with the release of Llama models and parameter-efficient techniques to fine-tune them (LoRA, QLoRA), this created a rich ecosystem of local LLMs that are now competing with OpenAI’s GPT-3.5 and GPT-4.\nCurrently, there are three main quantization techniques: NF4, GPTQ, and GGML. NF4 is a static method used by QLoRA to load a model in 4-bit precision to perform fine-tuning. In a previous article, we explored the GPTQ method and quantized our own model to run it on a consumer GPU. In this article, we will introduce the GGML technique, see how to quantize Llama models, and provide tips and tricks to achieve the best results.\nYou can find the code on Google Colab and GitHub.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "3. Quantization with GGML"
    ]
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#what-is-ggml",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#what-is-ggml",
    "title": "Quantize Llama models with GGUF and llama.cpp",
    "section": "What is GGML?",
    "text": "What is GGML?\nGGML is a C library focused on machine learning. It was created by Georgi Gerganov, which is what the initials “GG” stand for. This library not only provides foundational elements for machine learning, such as tensors, but also a unique binary format to distribute LLMs.\nThis format recently changed to GGUF. This new format is designed to be extensible, so that new features shouldn’t break compatibility with existing models. It also centralizes all the metadata in one file, such as special tokens, RoPE scaling parameters, etc. In short, it answers a few historical pain points and should be future-proof. For more information, you can read the specification at this address. In the rest of the article, we will call “GGML models” all models that either use GGUF or previous formats.\nGGML was designed to be used in conjunction with the llama.cpp library, also created by Georgi Gerganov. The library is written in C/C++ for efficient inference of Llama models. It can load GGML models and run them on a CPU. Originally, this was the main difference with GPTQ models, which are loaded and run on a GPU. However, you can now offload some layers of your LLM to the GPU with llama.cpp. To give you an example, there are 35 layers for a 7b parameter model. This drastically speeds up inference and allows you to run LLMs that don’t fit in your VRAM.\n\n\n\nIf command-line tools are your thing, llama.cpp and GGUF support have been integrated into many GUIs, like oobabooga’s text-generation-web-ui, koboldcpp, LM Studio, or ctransformers. You can simply load your GGML models with these tools and interact with them in a ChatGPT-like way. Fortunately, many quantized models are directly available on the Hugging Face Hub. You’ll quickly notice that most of them are quantized by TheBloke, a popular figure in the LLM community.\nIn the next section, we will see how to quantize our own models and run them on a consumer GPU.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "3. Quantization with GGML"
    ]
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#how-to-quantize-llms-with-ggml",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#how-to-quantize-llms-with-ggml",
    "title": "Quantize Llama models with GGUF and llama.cpp",
    "section": "How to quantize LLMs with GGML?",
    "text": "How to quantize LLMs with GGML?\nLet’s look at the files inside of TheBloke/Llama-2-13B-chat-GGML repo. We can see 14 different GGML models, corresponding to different types of quantization. They follow a particular naming convention: “q” + the number of bits used to store the weights (precision) + a particular variant. Here is a list of all the possible quant methods and their corresponding use cases, based on model cards made by TheBloke:\n\nq2_k: Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\nq3_k_l: Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\nq3_k_m: Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\nq3_k_s: Uses Q3_K for all tensors\nq4_0: Original quant method, 4-bit.\nq4_1: Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\nq4_k_m: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\nq4_k_s: Uses Q4_K for all tensors\nq5_0: Higher accuracy, higher resource usage and slower inference.\nq5_1: Even higher accuracy, resource usage and slower inference.\nq5_k_m: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\nq5_k_s: Uses Q5_K for all tensors\nq6_k: Uses Q8_K for all tensors\nq8_0: Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\n\nAs a rule of thumb, I recommend using Q5_K_M as it preserves most of the model’s performance. Alternatively, you can use Q4_K_M if you want to save some memory. In general, K_M versions are better than K_S versions. I cannot recommend Q2_K or Q3_* versions, as they drastically decrease model performance.\nNow that we know more about the quantization types available, let’s see how to use them on a real model. You can execute the following code on a free T4 GPU on Google Colab. The first step consists of compiling llama.cpp and installing the required libraries in our Python environment.\n\n# Install llama.cpp\n!git clone https://github.com/ggerganov/llama.cpp\n!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n!pip install -r llama.cpp/requirements.txt\n\nNow we can download our model. We will use the model we fine-tuned in this article, mlabonne/EvolCodeLlama-7b.\n\nMODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\n\n# Download model\n!git lfs install\n!git clone https://huggingface.co/{MODEL_ID}\n\nThis step can take a while. Once it’s done, we need to convert our weight to GGML FP16 format.\n\nMODEL_NAME = MODEL_ID.split('/')[-1]\n\n# Convert to fp16\nfp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n\nFinally, we can quantize the model using one or several methods. In this case, we will use the Q4_K_M and Q5_K_M methods I recommended earlier. This is the only step that actually requires a GPU.\n\nQUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n\nfor method in QUANTIZATION_METHODS:\n    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n    !./llama.cpp/quantize {fp16} {qtype} {method}\n\nOur two quantized models are now ready for inference. We can check the size of the bin files to see how much we compressed them. The FP16 model takes up 13.5 GB, while the Q4_K_M model takes up 4.08 GB (3.3 times smaller) and the Q5_K_M model takes up 4.78 GB (2.8 times smaller).\nLet’s use llama.cpp to efficiently run them. Since we’re using a GPU with 16 GB of VRAM, we can offload every layer to the GPU. In this case, it represents 35 layers (7b parameter model), so we’ll use the -ngl 35 parameter. In the following code block, we’ll also input a prompt and the quantization method we want to use.\n\nimport os\n\nmodel_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n\nprompt = input(\"Enter your prompt: \")\nchosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n\n# Verify the chosen method is in the list\nif chosen_method not in model_list:\n    print(\"Invalid name\")\nelse:\n    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\"\n\nLet’s ask the model “Write a Python function to print the nth Fibonacci numbers” using the Q5_K_M method. If we look at the logs, we can confirm that we successfully offloaded our layers thanks to the line “llm_load_tensors: offloaded 35/35 layers to GPU”. Here is the code the model generated:\ndef fib(n):\n    if n == 0 or n == 1:\n        return n\n    return fib(n - 2) + fib(n - 1)\n\nfor i in range(1, 10):\n    print(fib(i))\nThis wasn’t a very complex prompt, but it successfully produced a working piece of code in no time. With llama.cpp, you can use your local LLM as an assistant in a terminal using the interactive mode (-i flag). Note that this also works on Macbooks with Apple’s Metal Performance Shaders (MPS), which is an excellent option to run LLMs.\nFinally, we can push our quantized model to a new repo on the Hugging Face Hub with the “-GGUF” suffix. First, let’s log in and modify the following code block to match your username. You can enter your Hugging Face token (https://huggingface.co/settings/tokens) in Google Colab’s “Secrets” tab. We use the allow_patterns parameter to only upload GGUF models and not the entirety of the directory.\n\n!pip install -q huggingface_hub\nfrom huggingface_hub import create_repo, HfApi\nfrom google.colab import userdata\n\n# Defined in the secrets tab in Google Colab\nhf_token = userdata.get('huggingface')\n\napi = HfApi()\nusername = \"mlabonne\"\n\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n    repo_type=\"model\",\n    exist_ok=True,\n    token=hf_token\n)\n\n# Upload gguf files\napi.upload_folder(\n    folder_path=MODEL_NAME,\n    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n    allow_patterns=f\"*.gguf\",\n    token=hf_token\n)\n\nWe have successfully quantized, run, and pushed GGML models to the Hugging Face Hub! In the next section, we will explore how GGML actually quantize these models.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "3. Quantization with GGML"
    ]
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#quantization-with-ggml",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#quantization-with-ggml",
    "title": "Quantize Llama models with GGUF and llama.cpp",
    "section": "Quantization with GGML",
    "text": "Quantization with GGML\nThe way GGML quantizes weights is not as sophisticated as GPTQ’s. Basically, it groups blocks of values and rounds them to a lower precision. Some techniques, like Q4_K_M and Q5_K_M, implement a higher precision for critical layers. In this case, every weight is stored in 4-bit precision, with the exception of half of the attention.wv and feed_forward.w2 tensors. Experimentally, this mixed precision proves to be a good tradeoff between accuracy and resource usage.\nIf we look into the ggml.c file, we can see how the blocks are defined. For example, the block_q4_0 structure is defined as:\n#define QK4_0 32\ntypedef struct {\n    ggml_fp16_t d;          // delta\n    uint8_t qs[QK4_0 / 2];  // nibbles / quants\n} block_q4_0;\nIn GGML, weights are processed in blocks, each consisting of 32 values. For each block, a scale factor (delta) is derived from the largest weight value. All weights in the block are then scaled, quantized, and packed efficiently for storage (nibbles). This approach significantly reduces the storage requirements while allowing for a relatively simple and deterministic conversion between the original and quantized weights.\nNow that we know more about the quantization process, we can compare the results with NF4 and GPTQ.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "3. Quantization with GGML"
    ]
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#nf4-vs.-ggml-vs.-gptq",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#nf4-vs.-ggml-vs.-gptq",
    "title": "Quantize Llama models with GGUF and llama.cpp",
    "section": "NF4 vs. GGML vs. GPTQ",
    "text": "NF4 vs. GGML vs. GPTQ\nWhich technique is better for 4-bit quantization? To answer this question, we need to introduce the different backends that run these quantized LLMs. For GGML models, llama.cpp with Q4_K_M models is the way to go. For GPTQ models, we have two options: AutoGPTQ or ExLlama. Finally, NF4 models can directly be run in transformers with the --load-in-4bit flag.\nOobabooga ran multiple experiments in an excellent blog post that compare different models in terms of perplexity (lower is better):\n\n\n\nBased on these results, we can say that GGML models have a slight advantage in terms of perplexity. The difference is not particularly significant, which is why it is better to focus on the generation speed in terms of tokens/second. The best technique depends on your GPU: if you have enough VRAM to fit the entire quantized model, GPTQ with ExLlama will be the fastest. If that’s not the case, you can offload some layers and use GGML models with llama.cpp to run your LLM.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "3. Quantization with GGML"
    ]
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#conclusion",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#conclusion",
    "title": "Quantize Llama models with GGUF and llama.cpp",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we introduced the GGML library and the new GGUF format to efficiently store these quantized models. We used it to quantize our own Llama model in different formats (Q4_K_M and Q5_K_M). We then ran the GGML model and pushed our bin files to the Hugging Face Hub. Finally, we delved deeper into GGML’s code to understand how it actually quantizes the weights and compared it to NF4 and GPTQ.\nQuantization is a formidable vector to democratize LLMs by lowering the cost of running them. In the future, mixed precision and other techniques will keep improving the performance we can achieve with quantized weights. Until then, I hope you enjoyed reading this article and learned something new.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "3. Quantization with GGML"
    ]
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html",
    "href": "posts/Introduction_to_Weight_Quantization.html",
    "title": "Introduction to Weight Quantization",
    "section": "",
    "text": "Large Language Models (LLMs) are known for their extensive computational requirements. Typically, the size of a model is calculated by multiplying the number of parameters (size) by the precision of these values (data type). However, to save memory, weights can be stored using lower-precision data types through a process known as quantization.\nWe distinguish two main families of weight quantization techniques in the literature:\nIn this article, we focus on PTQ to reduce the precision of our parameters. To get a good intuition, we will apply both naïve and more sophisticated techniques to a toy example using a GPT-2 model.\nThe entire code is freely available on Google Colab and GitHub.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "1. Intro to Quantization"
    ]
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation",
    "href": "posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation",
    "title": "Introduction to Weight Quantization",
    "section": "📚 Background on Floating Point Representation",
    "text": "📚 Background on Floating Point Representation\nThe choice of data type dictates the quantity of computational resources required, affecting the speed and efficiency of the model. In deep learning applications, balancing precision and computational performance becomes a vital exercise as higher precision often implies greater computational demands.\nAmong various data types, floating point numbers are predominantly employed in deep learning due to their ability to represent a wide range of values with high precision. Typically, a floating point number uses \\(n\\) bits to store a numerical value. These \\(n\\) bits are further partitioned into three distinct components:\n\nSign: The sign bit indicates the positive or negative nature of the number. It uses one bit where 0 indicates a positive number and 1 signals a negative number.\nExponent: The exponent is a segment of bits that represents the power to which the base (usually 2 in binary representation) is raised. The exponent can also be positive or negative, allowing the number to represent very large or very small values.\nSignificand/Mantissa: The remaining bits are used to store the significand, also referred to as the mantissa. This represents the significant digits of the number. The precision of the number heavily depends on the length of the significand.\n\nThis design allows floating point numbers to cover a wide range of values with varying levels of precision. The formula used for this representation is:\n\\[(-1)^{\\text{sign}} \\times \\text{base}^{\\text{exponent}} \\times \\text{significand}\\]\nTo understand this better, let’s delve into some of the most commonly used data types in deep learning: float32 (FP32), float16 (FP16), and bfloat16 (BF16):\n\nFP32 uses 32 bits to represent a number: one bit for the sign, eight for the exponent, and the remaining 23 for the significand. While it provides a high degree of precision, the downside of FP32 is its high computational and memory footprint.\nFP16 uses 16 bits to store a number: one is used for the sign, five for the exponent, and ten for the significand. Although this makes it more memory-efficient and accelerates computations, the reduced range and precision can introduce numerical instability, potentially impacting model accuracy.\nBF16 is also a 16-bit format but with one bit for the sign, eight for the exponent, and seven for the significand. BF16 expands the representable range compared to FP16, thus decreasing underflow and overflow risks. Despite a reduction in precision due to fewer significand bits, BF16 typically does not significantly impact model performance and is a useful compromise for deep learning tasks.\n\n\n\n\n\n\nIn ML jargon, FP32 is often termed “full precision” (4 bytes), while BF16 and FP16 are “half-precision” (2 bytes). But could we do even better and store weights using a single byte? The answer is the INT8 data type, which consists of an 8-bit representation capable of storing \\(2^8 = 256\\) different values. In the next section, we’ll see how to convert FP32 weights into an INT8 format.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "1. Intro to Quantization"
    ]
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#naïve-8-bit-quantization",
    "href": "posts/Introduction_to_Weight_Quantization.html#naïve-8-bit-quantization",
    "title": "Introduction to Weight Quantization",
    "section": "🔰 Naïve 8-bit Quantization",
    "text": "🔰 Naïve 8-bit Quantization\nIn this section, we will implement two quantization techniques: a symmetric one with absolute maximum (absmax) quantization and an asymmetric one with zero-point quantization. In both cases, the goal is to map an FP32 tensor \\(\\mathbf{X}\\) (original weights) to an INT8 tensor \\(\\mathbf{X}_{\\text{quant}}\\) (quantized weights).\nWith absmax quantization, the original number is divided by the absolute maximum value of the tensor and multiplied by a scaling factor (127) to map inputs into the range [-127, 127]. To retrieve the original FP16 values, the INT8 number is divided by the quantization factor, acknowledging some loss of precision due to rounding.\n\\[\\begin{align*}\n\\mathbf{X}_{\\text{quant}} &= \\text{round}\\Biggl ( \\frac{127}{\\max|\\mathbf{X}|} \\cdot \\mathbf{X} \\Biggr ) \\\\\n\\mathbf{X}_{\\text{dequant}} &= \\frac{\\max|\\mathbf{X}|}{127} \\cdot \\mathbf{X}_{\\text{quant}}\n\\end{align*}\\]\nFor instance, let’s say we have an absolution maximum value of 3.2. A weight of 0.1 would be quantized to \\(\\text{round}(0.1 \\times \\frac{127}{3.2}) = 4\\). If we want to dequantize it, we would get \\(4 / \\frac{127}{3.2} = 0.1008\\), which implies an error of 0.008. Here’s the corresponding Python implementation:\n\nimport torch\n\ndef absmax_quantize(X):\n    # Calculate scale\n    scale = 127 / torch.max(torch.abs(X))\n\n    # Quantize\n    X_quant = (scale * X).round()\n\n    # Dequantize\n    X_dequant = X_quant / scale\n\n    return X_quant.to(torch.int8), X_dequant\n\nWith zero-point quantization, we can consider asymmetric input distributions, which is useful when you consider the output of a ReLU function (only positive values) for example. The input values are first scaled by the total range of values (255) divided by the difference between the maximum and minimum values. This distribution is then shifted by the zero-point to map it into the range [-128, 127] (notice the extra value compared to absmax). First, we calculate the scale factor and the zero-point value:\n\\[\\begin{align*}\n\\text{scale} &= \\frac{255}{\\max(\\mathbf{X}) - \\min(\\mathbf{X})} \\\\\n\\text{zeropoint} &= - \\text{round}(\\text{scale} \\cdot \\min(\\mathbf{X})) - 128\n\\end{align*}\\]\nThen, we can use these variables to quantize or dequantize our weights:\n\\[\\begin{align*}\n\\mathbf{X}_{\\text{quant}} &= \\text{round}\\bigg(\\text{scale} \\cdot \\mathbf{X} + \\text{zeropoint} \\bigg) \\\\\n\\mathbf{X}_{\\text{dequant}} &= \\frac{\\mathbf{X}_{\\text{quant}} - \\text{zeropoint}}{\\text{scale}}\n\\end{align*}\\]\nLet’s take an example: we have a maximum value of 3.2 and a minimum value of -3.0. We can calculate the scale is \\(\\frac{255}{3.2 + 3.0} = 41.13\\) and the zero-point \\(-\\text{round}(41.13 \\cdot -3.0) - 128 = 123 - 128 = -5\\), so our previous weight of 0.1 would be quantized to \\(\\text{round}(41.13 \\cdot 0.1 - 5) = -1\\). This is very different from the previous value obtained using absmax (4 vs. -1).\n\n\n\n\n\nThe Python implementation is quite straightforward:\n\ndef zeropoint_quantize(X):\n    # Calculate value range (denominator)\n    x_range = torch.max(X) - torch.min(X)\n    x_range = 1 if x_range == 0 else x_range\n\n    # Calculate scale\n    scale = 255 / x_range\n\n    # Shift by zero-point\n    zeropoint = (-scale * torch.min(X) - 128).round()\n\n    # Scale and round the inputs\n    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n\n    # Dequantize\n    X_dequant = (X_quant - zeropoint) / scale\n\n    return X_quant.to(torch.int8), X_dequant\n\nInstead of relying on complete toy examples, we can use these two functions on a real model thanks to the transformers library.\nWe start by loading the model and tokenizer for GPT-2. This is a very small model we probably don’t want to quantize, but it will be good enough for this tutorial. First, we want to observe the model’s size so we can compare it later and evaluate the memory savings due to 8-bit quantization.\n\n!pip install -q bitsandbytes&gt;=0.39.0\n!pip install -q git+https://github.com/huggingface/accelerate.git\n!pip install -q git+https://github.com/huggingface/transformers.git\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(0)\n\n# Set device to CPU for now\ndevice = 'cpu'\n\n# Load model and tokenizer\nmodel_id = 'gpt2'\nmodel = AutoModelForCausalLM.from_pretrained(model_id).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Print model size\nprint(f\"Model size: {model.get_memory_footprint():,} bytes\")\n\n\n\n\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\nCUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\n\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-20b5bv2xvtu9a --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\nEither way, this might cause trouble in the future:\nIf you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n  warn(msg)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel size: 510,342,192 bytes\n\n\nThe size of the GPT-2 model is approximately 487MB in FP32. The next step consists of quantizing the weights using zero-point and absmax quantization. In the following example, we apply these techniques to the first attention layer of GPT-2 to see the results.\n\n# Extract weights of the first layer\nweights = model.transformer.h[0].attn.c_attn.weight.data\nprint(\"Original weights:\")\nprint(weights)\n\n# Quantize layer using absmax quantization\nweights_abs_quant, _ = absmax_quantize(weights)\nprint(\"\\nAbsmax quantized weights:\")\nprint(weights_abs_quant)\n\n# Quantize layer using absmax quantization\nweights_zp_quant, _ = zeropoint_quantize(weights)\nprint(\"\\nZero-point quantized weights:\")\nprint(weights_zp_quant)\n\nOriginal weights:\ntensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n        ...,\n        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])\n\nAbsmax quantized weights:\ntensor([[-21, -12,  -4,  ...,   2,  -3,   1],\n        [  4,   7,  11,  ...,  -2,  -1,  -1],\n        [  0,   3,  16,  ...,   5,   2,  -1],\n        ...,\n        [-12,  -1,   9,  ...,   0,  -2,   1],\n        [  7,  10,   5,  ...,   1,  -2,  -2],\n        [-18,  -9, -11,  ...,   0,   0,   1]], dtype=torch.int8)\n\nZero-point quantized weights:\ntensor([[-20, -11,  -3,  ...,   3,  -2,   2],\n        [  5,   8,  12,  ...,  -1,   0,   0],\n        [  1,   4,  18,  ...,   6,   3,   0],\n        ...,\n        [-11,   0,  10,  ...,   1,  -1,   2],\n        [  8,  11,   6,  ...,   2,  -1,  -1],\n        [-18,  -8, -10,  ...,   1,   1,   2]], dtype=torch.int8)\n\n\nThe difference between the original (FP32) and quantized values (INT8) is clear, but the difference between absmax and zero-point weights is more subtle. In this case, the inputs look shifted by a value of -1. This suggests that the weight distribution in this layer is quite symmetric.\nWe can compare these techniques by quantizing every layer in GPT-2 (linear layers, attention layers, etc.) and create two new models: model_abs and model_zp. To be precise, we will actually replace the original weights with de-quantized ones. This has two benefits: it allows us to 1/ compare the distribution of our weights (same scale) and 2/ actually run the models.\nIndeed, PyTorch doesn’t allow INT8 matrix multiplication by default. In a real scenario, we would dequantize them to run the model (in FP16 for example) but store them as INT8. In the next section, we will use the bitsandbytes library to solve this issue.\n\nimport numpy as np\nfrom copy import deepcopy\n\n# Store original weights\nweights = [param.data.clone() for param in model.parameters()]\n\n# Create model to quantize\nmodel_abs = deepcopy(model)\n\n# Quantize all model weights\nweights_abs = []\nfor param in model_abs.parameters():\n    _, dequantized = absmax_quantize(param.data)\n    param.data = dequantized\n    weights_abs.append(dequantized)\n\n# Create model to quantize\nmodel_zp = deepcopy(model)\n\n# Quantize all model weights\nweights_zp = []\nfor param in model_zp.parameters():\n    _, dequantized = zeropoint_quantize(param.data)\n    param.data = dequantized\n    weights_zp.append(dequantized)\n\nNow that our models have been quantized, we want to check the impact of this process. Intuitively, we want to make sure that the quantized weights are close to the original ones. A visual way to check it is to plot the distribution of the dequantized and original weights. If the quantization is lossy, it would drastically change the weight distribution.\nThe following figure shows this comparison, where the blue histogram represents the original (FP32) weights, and the red one represents the dequantized (from INT8) weights. Note that we only display this plot between -2 and 2 because of outliers with very high absolute values (more on that later).\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Flatten weight tensors\nweights = np.concatenate([t.cpu().numpy().flatten() for t in weights])\nweights_abs = np.concatenate([t.cpu().numpy().flatten() for t in weights_abs])\nweights_zp = np.concatenate([t.cpu().numpy().flatten() for t in weights_zp])\n\n# Set background style\nplt.style.use('ggplot')\n\n# Create figure and axes\nfig, axs = plt.subplots(2, figsize=(10,10), dpi=300, sharex=True)\n\n# Plot the histograms for original and zero-point weights\naxs[0].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\naxs[0].hist(weights_abs, bins=150, alpha=0.5, label='Absmax weights', color='red', range=(-2, 2))\n\n# Plot the histograms for original and absmax weights\naxs[1].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\naxs[1].hist(weights_zp, bins=150, alpha=0.5, label='Zero-point weights', color='green', range=(-2, 2))\n\n# Add grid\nfor ax in axs:\n    ax.grid(True, linestyle='--', alpha=0.6)\n\n# Add legend\naxs[0].legend()\naxs[1].legend()\n\n# Add title and labels\naxs[0].set_title('Comparison of Original and Absmax Quantized Weights', fontsize=16)\naxs[1].set_title('Comparison of Original and Zeropoint Quantized Weights', fontsize=16)\n\nfor ax in axs:\n    ax.set_xlabel('Weights', fontsize=14)\n    ax.set_ylabel('Count', fontsize=14)\n    ax.yaxis.set_major_formatter(ticker.EngFormatter()) # Make y-ticks more human readable\n\n# Improve font\nplt.rc('font', size=12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBoth plots are quite similar, with a surprising spike around 0. This spike shows that our quantization is quite lossy since reversing the process doesn’t output the original values. This is particularly true for the absmax model, which displays both a lower valley and a higher spike around 0.\nLet’s compare the performance of the original and quantized models. For this purpose, we define a generate_text() function to generate 50 tokens with top-k sampling.\n\ndef generate_text(model, input_text, max_length=50):\n    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n    output = model.generate(inputs=input_ids,\n                            max_length=max_length,\n                            do_sample=True,\n                            top_k=30,\n                            pad_token_id=tokenizer.eos_token_id,\n                            attention_mask=input_ids.new_ones(input_ids.shape))\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Generate text with original and quantized models\noriginal_text = generate_text(model, \"I have a dream\")\nabsmax_text   = generate_text(model_abs, \"I have a dream\")\nzp_text       = generate_text(model_zp, \"I have a dream\")\n\nprint(f\"Original model:\\n{original_text}\")\nprint(\"-\" * 50)\nprint(f\"Absmax model:\\n{absmax_text}\")\nprint(\"-\" * 50)\nprint(f\"Zeropoint model:\\n{zp_text}\")\n\nOriginal model:\nI have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the\n--------------------------------------------------\nAbsmax model:\nI have a dream to find out the origin of her hair. She loves it. But there's no way you could be honest about how her hair is made. She must be crazy.\n\nWe found a photo of the hairstyle posted on\n--------------------------------------------------\nZeropoint model:\nI have a dream of creating two full-time jobs in America—one for people with mental health issues, and one for people who do not suffer from mental illness—or at least have an employment and family history of substance abuse, to work part\n\n\nInstead of trying to see if one output makes more sense than the others, we can quantify it by calculating the perplexity of each output. This is a common metric used to evaluate language models, which measures the uncertainty of a model in predicting the next token in a sequence. In this comparison, we make the common assumption that the lower the score, the better the model is. In practice, a sentence with a high perplexity could also be correct.\nWe implement it using a minimal function since it doesn’t need to consider details like the length of the context window since our sentences are short.\n\ndef calculate_perplexity(model, text):\n    # Encode the text\n    encodings = tokenizer(text, return_tensors='pt').to(device)\n\n    # Define input_ids and target_ids\n    input_ids = encodings.input_ids\n    target_ids = input_ids.clone()\n\n    with torch.no_grad():\n        outputs = model(input_ids, labels=target_ids)\n\n    # Loss calculation\n    neg_log_likelihood = outputs.loss\n\n    # Perplexity calculation\n    ppl = torch.exp(neg_log_likelihood)\n\n    return ppl\n\nppl     = calculate_perplexity(model, original_text)\nppl_abs = calculate_perplexity(model_abs, absmax_text)\nppl_zp  = calculate_perplexity(model_zp, absmax_text)\n\nprint(f\"Original perplexity:  {ppl.item():.2f}\")\nprint(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")\nprint(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")\n\nOriginal perplexity: 15.53\nAbsmax perplexity:   17.92\nZeropoint perplexity: 17.97\n\n\nWe see that the perplexity of the original model is slightly lower than the two others. A single experiment is not very reliable, but we could repeat this process multiple times to see the difference between each model. In theory, zero-point quantization should be slightly better than absmax, but is also more costly to compute.\nIn this example, we applied quantization techniques to entire layers (per-tensor basis). However, we could apply it at different granularity levels: from the entire model to individual values. Quantizing the entire model in one pass would seriously degrade the performance, while quantizing individual values would create a big overhead. In practice, we often prefer the vector-wise quantization, which considers the variability of values in rows and columns inside of the same tensor.\nHowever, even vector-wise quantization doesn’t solve the problem of outlier features. Outlier features are extreme values (negative or positive) that appear in all transformer layers when the model reach a certain scale (&gt;6.7B parameters). This is an issue since a single outlier can reduce the precision for all other values. But discarding these outlier features is not an option since it would greatly degrade the model’s performance.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "1. Intro to Quantization"
    ]
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#bit-quantization-with-llm.int8",
    "href": "posts/Introduction_to_Weight_Quantization.html#bit-quantization-with-llm.int8",
    "title": "Introduction to Weight Quantization",
    "section": "🔢 8-bit Quantization with LLM.int8()",
    "text": "🔢 8-bit Quantization with LLM.int8()\nIntroduced by Dettmers et al. (2022), LLM.int8() is a solution to the outlier problem. It relies on a vector-wise (absmax) quantization scheme and introduces mixed-precision quantization. This means that outlier features are processed in a FP16 format to retain their precision, while the other values are processed in an INT8 format. As outliers represent about 0.1% of values, this effectively reduces the memory footprint of the LLM by almost 2x.\n\n\n\nLLM.int8() works by conducting matrix multiplication computation in three key steps:\n\nExtract columns from the input hidden states \\(\\mathbf{X}\\) containing outlier features using a custom threshold.\nPerform the matrix multiplication of the outliers using FP16 and the non-outliers using INT8 with vector-wise quantization (row-wise for the hidden state \\(\\mathbf{X}\\) and column-wise for the weight matrix \\(\\mathbf{W}\\)).\nDequantize the non-outlier results (INT8 to FP16) and add them to the outlier results to get the full result in FP16.\n\n\n\n\nThis approach is necessary because 8-bit precision is limited and can lead to substantial errors when quantizing a vector with large values. These errors also tend to amplify as they propagate through multiple layers.\nWe can easily use this technique thanks to the integration of the bitsandbytes library into the Hugging Face ecosystem. We just need to specify load_in_8bit=True when loading the model (it also requires a GPU).\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n                                             device_map='auto',\n                                             load_in_8bit=True,\n                                             )\nprint(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")\n\nSome weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nModel size: 176,527,896 bytes\n\n\nWith this extra line of code, the model is now almost three times smaller (168MB vs. 487MB). We can even compare the distribution of the original and quantized weights as we did earlier:\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Flatten weight tensors\nweights_int8 = [param.data.clone() for param in model_int8.parameters()]\nweights_int8 = np.concatenate([t.cpu().numpy().flatten() for t in weights_int8])\n\n# Set background style\nplt.style.use('ggplot')\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(10,5), dpi=300)\n\n# Plot the histograms\nax.hist(weights, bins=150, alpha=0.5, label='Original weights',\n        color='blue', range=(-2, 2))\nax.hist(weights_int8, bins=150, alpha=0.5, label='LLM.int8() weights',\n        color='red', range=(-2, 2))\n\n# Add grid\nax.grid(True, linestyle='--', alpha=0.6)\n\n# Add legend\nax.legend()\n\n# Add title and labels\nax.set_title('Comparison of Original and Dequantized Weights', fontsize=16)\nax.set_xlabel('Weights', fontsize=14)\nax.set_ylabel('Count', fontsize=14)\nplt.gca().yaxis.set_major_formatter(ticker.EngFormatter())\n\n# Improve font\nplt.rc('font', size=12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn this case, we see spikes around -2, -1, 0, 1, 2, etc. These values correspond to the parameters stored in the INT8 format (non-outliers). You can verify it by printing the model’s weights using model_int8.parameters().\nWe can also generate text with this quantized model and compare it to the original model.\n\n# Generate text with quantized model\ntext_int8 = generate_text(model_int8, \"I have a dream\")\n\nprint(f\"Original model:\\n{original_text}\")\nprint(\"-\" * 50)\nprint(f\"LLM.int8() model:\\n{text_int8}\")\n\nOriginal model:\nI have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the\n--------------------------------------------------\nLLM.int8() model:\nI have a dream. I don't know what will come of it, but I am going to have to look for something that will be right. I haven't thought about it for a long time, but I have to try to get that thing\n\n\nOnce again, it is difficult to judge what is the best output, but we can rely on the perplexity metric to give us an (approximate) answer.\n\nprint(f\"Perplexity (original):   {ppl.item():.2f}\")\n\nppl = calculate_perplexity(model_int8, text_int8)\nprint(f\"Perplexity (LLM.int8()): {ppl.item():.2f}\")\n\nPerplexity (original):   15.53\nPerplexity (LLM.int8()): 7.93\n\n\nIn this case, the perplexity of the quantized model is twice as low as the original one. In general, this is not the case, but it shows that this quantization technique is very competitive. In fact, the authors of LLM.int8() show that the performance degradation is so low it’s negligible (&lt;1%). However, it has an additional cost in terms of computation: LLM.int8() is roughly about 20% slower for large models.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "1. Intro to Quantization"
    ]
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#conclusion",
    "href": "posts/Introduction_to_Weight_Quantization.html#conclusion",
    "title": "Introduction to Weight Quantization",
    "section": "Conclusion",
    "text": "Conclusion\nThis article provided an overview of the most popular weight quantization techniques. We started by gaining an understanding of floating point representation, before introducing two techniques for 8-bit quantization: absmax and zero-point quantization. However, their limitations, particularly when it comes to handling outliers, led to LLM.int8(), a technique that also preserves the model’s performance. This approach underlines the progress being made in the field of weight quantization, revealing the importance of properly addressing outliers.\nLooking forward, our next article will explore the GPTQ weight quantization technique in depth. This technique, introduced by Frantar et al., only utilizes 4 bits and represents a significant advancement in the field of weight quantization. We will provide a comprehensive guide on how to implement GPTQ using the AutoGPTQ library.\nIf you’re interested in more technical content around LLMs, follow me on Twitter @maximelabonne.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "1. Intro to Quantization"
    ]
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#references",
    "href": "posts/Introduction_to_Weight_Quantization.html#references",
    "title": "Introduction to Weight Quantization",
    "section": "References",
    "text": "References\n\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. 2022.\nY. Beldaka, and T. Dettmers, A Gentle Introduction to 8-bit Matrix Multiplication, Hugging Face Blog (2022).\nA. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, A Survey of Quantization Methods for Efficient Neural Network Inference. 2021.\nH. Wu, P. Judd, X. Zhang, M. Isaev, and P. Micikevicius, Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation. 2020.\nLilian Weng, Large Transformer Model Inference Optimization, Lil’Log (2023).\nKamil Czarnogorski, Local Large Language Models, Int8 (2023).",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "1. Intro to Quantization"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "",
    "text": "With the release of LLaMA v1, we saw a Cambrian explosion of fine-tuned models, including Alpaca, Vicuna, WizardLM, among others. This trend encouraged different businesses to launch their own base models with licenses suitable for commercial use, such as OpenLLaMA, Falcon, XGen, etc. The release of Llama 2 now combines the best elements from both sides: it offers a highly efficient base model along with a more permissive license.\nDuring the first half of 2023, the software landscape was significantly shaped by the widespread use of APIs (like OpenAI API) to create infrastructures based on Large Language Models (LLMs). Libraries such as LangChain and LlamaIndex played a critical role in this trend. Moving into the latter half of the year, the process of fine-tuning (or instruction tuning) these models is set to become a standard procedure in the LLMOps workflow. This trend is driven by various factors: the potential for cost savings, the ability to process confidential data, and even the potential to develop models that exceed the performance of prominent models like ChatGPT and GPT-4 in certain specific tasks.\nIn this article, we will see why instruction tuning works and how to implement it in a Google Colab notebook to create your own Llama 2 model. As usual, the code is available on Colab and GitHub.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#background-on-fine-tuning-llms",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#background-on-fine-tuning-llms",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "🔧 Background on fine-tuning LLMs",
    "text": "🔧 Background on fine-tuning LLMs\n\n\n\n\nLLMs are pretrained on an extensive corpus of text. In the case of Llama 2, we know very little about the composition of the training set, besides its length of 2 trillion tokens. In comparison, BERT (2018) was “only” trained on the BookCorpus (800M words) and English Wikipedia (2,500M words). From experience, this is a very costly and long process with a lot of hardware issues. If you want to know more about it, I recommend reading Meta’s logbook about the pretraining of the OPT-175B model.\nWhen the pretraining is complete, auto-regressive models like Llama 2 can predict the next token in a sequence. However, this does not make them particularly useful assistants since they don’t reply to instructions. This is why we employ instruction tuning to align their answers with what humans expect. There are two main fine-tuning techniques:\n\nSupervised Fine-Tuning (SFT): Models are trained on a dataset of instructions and responses. It adjusts the weights in the LLM to minimize the difference between the generated answers and ground-truth responses, acting as labels.\nReinforcement Learning from Human Feedback (RLHF): Models learn by interacting with their environment and receiving feedback. They are trained to maximize a reward signal (using PPO), which is often derived from human evaluations of model outputs.\n\nIn general, RLHF is shown to capture more complex and nuanced human preferences, but is also more challenging to implement effectively. Indeed, it requires careful design of the reward system and can be sensitive to the quality and consistency of human feedback. A possible alternative in the future is the Direct Preference Optimization (DPO) algorithm, which directly runs preference learning on the SFT model.\nIn our case, we will perform SFT, but this raises a question: why does fine-tuning work in the first place? As highlighted in the Orca paper, our understanding is that fine-tuning leverages knowledge learned during the pretraining process. In other words, fine-tuning will be of little help if the model has never seen the kind of data you’re interested in. However, if that’s the case, SFT can be extremely performant.\nFor example, the LIMA paper showed how you could outperform GPT-3 (DaVinci003) by fine-tuning a LLaMA (v1) model with 65 billion parameters on only 1,000 high-quality samples. The quality of the instruction dataset is essential to reach this level of performance, which is why a lot of work is focused on this issue (like evol-instruct, Orca, or phi-1). Note that the size of the LLM (65b, not 13b or 7b) is also fundamental to leverage pre-existing knowledge efficiently.\nAnother important point related to the data quality is the prompt template. Prompts are comprised of similar elements: system prompt (optional) to guide the model, user prompt (required) to give the instruction, additional inputs (optional) to take into consideration, and the model’s answer (required). In the case of Llama 2, the authors used the following template for the chat models:\n&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nSystem prompt\n&lt;&lt;/SYS&gt;&gt;\n\nUser prompt [/INST] Model answer &lt;/s&gt;\nThere are other templates, like the ones from Alpaca and Vicuna, and their impact is not very clear. In this example, we will reformat our instruction dataset to follow Llama 2’s template. For the purpose of this tutorial, I’ve already done it using the excellent timdettmers/openassistant-guanaco dataset. You can find it on Hugging Face under the name mlabonne/guanaco-llama2-1k. In the following, we will use a base model instead of a chat model, so this step is optional. Note that you don’t need to follow a specific prompt template if you’re using the base Llama 2 model instead of the chat version.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#how-to-fine-tune-llama-2",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#how-to-fine-tune-llama-2",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "🦙 How to fine-tune Llama 2",
    "text": "🦙 How to fine-tune Llama 2\nIn this section, we will fine-tune a Llama 2 model with 7 billion parameters on a T4 GPU with high RAM using Google Colab (2.21 credits/hour). Note that a T4 only has 16 GB of VRAM, which is barely enough to store Llama 2-7b’s weights (7b × 2 bytes = 14 GB in FP16). In addition, we need to consider the overhead due to optimizer states, gradients, and forward activations (see this excellent article for more information). This means that a full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA.\nTo drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here. The good thing is that we can leverage the Hugging Face ecosystem with the transformers, accelerate, peft, trl, and bitsandbytes libraries. This is what we’ll do in the following code, based on Younes Belkada’s GitHub Gist. First, we install and load these libraries.\n\n!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\n\nLet’s talk a bit about the parameters we can tune here. First, we want to load a llama-2-7b-chat-hf model and train it on the mlabonne/guanaco-llama2-1k (1,000 samples), which will produce our fine-tuned model llama-2-7b-miniguanaco. If you’re interested in how this dataset was created, you can check this notebook. Feel free to change it: there are many good datasets on the Hugging Face Hub, like databricks/databricks-dolly-15k.\nQLoRA will use a rank of 64 with a scaling parameter of 16 (see this article for more information about LoRA parameters). We’ll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for 1 epoch. To get more information about the other parameters, check the TrainingArguments, PeftModel, and SFTTrainer documentation.\n\n# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\n\n# Fine-tuned model name\nnew_model = \"llama-2-7b-miniguanaco\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 25\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}\n\nWe can now load everything and start the fine-tuning process. We’re relying on multiple wrappers, so bear with me.\n\nFirst of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\nThen, we’re configuring bitsandbytes for 4-bit quantization.\nNext, we’re loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\nFinally, we’re loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!\n\n\n# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major &gt;= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()\n\n# Save trained model\ntrainer.model.save_pretrained(new_model)\n\n\n\n\nThe training can be very long, depending on the size of your dataset. Here, it took less than an hour on a T4 GPU. We can check the plots on tensorboard, as follows:\n\n%load_ext tensorboard\n%tensorboard --logdir results/runs\n\n\n\n\n\n\n\nLet’s make sure that the model is behaving correctly. It would require a more exhaustive evaluation, but we can use the text generation pipeline to ask questions like “What is a large language model?” Note that I’m formatting the input to match Llama 2’s prompt template.\n\n# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"What is a large language model?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"&lt;s&gt;[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])\n\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n\n\n&lt;s&gt;[INST] What is a large language model? [/INST] A large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human-like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural-sounding language generation.\n\nLarge language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music.\n\nLarge language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given\n\n\nThe model outputs the following response:\nA large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human-like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural-sounding language generation.\n\nLarge language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music.\n\nLarge language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given\nFrom experience, it is very coherent for a model with only 7 billion parameters. You can play with it and ask harder questions from evaluation datasets like BigBench-Hard. Guanaco is an excellent dataset that has produced high-quality models in the past. You can train a Llama 2 model on the entire dataset using mlabonne/guanaco-llama2.\nHow can we store our new llama-2-7b-miniguanaco model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything. Alas, it also creates a problem with the VRAM (despite emptying it), so I recommend restarting the notebook, re-executing the three first cells, and then executing the next one. Please contact me if you know a fix!\n\n# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n\n\n\nOur weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model.\n\n!huggingface-cli login\n\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/mlabonne/llama-2-7b-guanaco/commit/0f5ed9581b805b659aec68484820edb5e3e6c3f5', commit_message='Upload tokenizer', commit_description='', oid='0f5ed9581b805b659aec68484820edb5e3e6c3f5', pr_url=None, pr_revision=None, pr_num=None)\n\n\nYou can now use this model for inference by loading it like any other Llama 2 model from the Hub. It is also possible to reload it for more fine-tuning – perhaps with another dataset?\nIf you’re serious about fine-tuning models, using a script instead of a notebook is recommended. You can easily rent GPUs on Lambda Labs, Runpod, Vast.ai, for less than 0.3$/h. Once you’re connected, you can install libraries, import your script, log in to Hugging Face and other tools (like Weights & Biases for logging your experiments), and start your fine-tuning.\nThe trl script is currently very limited, so I made my own based on the previous notebook. You can find it here on GitHub Gist. If you’re looking for a comprehensive solution, check out Axolotl from the OpenAccess AI Collective, which natively handles multiple datasets, Deepspeed, Flash Attention, etc.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#conclusion",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#conclusion",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we saw how to fine-tune a Llama 2 7b model using a Colab notebook. We introduced some necessary background on LLM training and fine-tuning, as well as important considerations related to instruction datasets. In the second section, we successfully fine-tuned the Llama 2 model with its native prompt template and custom parameters.\nThese fine-tuned models can then be integrated into LangChain and other architectures as advantageous alternatives to the OpenAI API. Remember, in this new paradigm, instruction datasets are the new gold, and the quality of your model heavily depends on the data on which it’s been fine-tuned. So, good luck with building high-quality datasets!\nIf you’re interested in more content about LLMs, follow me on Twitter @maximelabonne.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#references",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#references",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "References",
    "text": "References\n\nHugo Touvron, Thomas Scialom, et al. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models.\nPhilipp Schmid, Omar Sanseviero, Pedro Cuenca, & Lewis Tunstall. Llama 2 is here - get it on Hugging Face. https://huggingface.co/blog/llama2\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, & Tatsunori B. Hashimoto. (2023). Stanford Alpaca: An Instruction-following LLaMA model.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, & Kristina Toutanova. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, & Luke Zettlemoyer. (2023). QLoRA: Efficient Finetuning of Quantized LLMs.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": " Research notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/ExLlamaV2_The_Fastest_Library_to_Run LLMs.html",
    "href": "posts/ExLlamaV2_The_Fastest_Library_to_Run LLMs.html",
    "title": "ExLlamaV2: The Fastest Library to Run LLMs",
    "section": "",
    "text": "Quantizing Large Language Models (LLMs) is the most popular approach to reduce the size of these models and speed up inference. Among these techniques, GPTQ delivers amazing performance on GPUs. Compared to unquantized models, this method uses almost 3 times less VRAM while providing a similar level of accuracy and faster generation. It became so popular that it has recently been directly integrated into the transformers library.\nExLlamaV2 is a library designed to squeeze even more performance out of GPTQ. Thanks to new kernels, it’s optimized for (blazingly) fast inference. It also introduces a new quantization format, EXL2, which brings a lot of flexibility to how weights are stored.\nIn this article, we will see how to quantize base models in the EXL2 format and how to run them. As usual, the code is available on GitHub and Google Colab."
  },
  {
    "objectID": "posts/ExLlamaV2_The_Fastest_Library_to_Run LLMs.html#quantize-exl2-models",
    "href": "posts/ExLlamaV2_The_Fastest_Library_to_Run LLMs.html#quantize-exl2-models",
    "title": "ExLlamaV2: The Fastest Library to Run LLMs",
    "section": "⚡ Quantize EXL2 models",
    "text": "⚡ Quantize EXL2 models\nTo start our exploration, we need to install the ExLlamaV2 library. In this case, we want to be able to use some scripts contained in the repo, which is why we will install it from source as follows:\ngit clone https://github.com/turboderp/exllamav2\npip install exllamav2\nNow that ExLlamaV2 is installed, we need to download the model we want to quantize in this format. Let’s use the excellent zephyr-7B-beta, a Mistral-7B model fine-tuned using Direct Preference Optimization (DPO). It claims to outperform Llama-2 70b chat on the MT bench, which is an impressive result for a model that is ten times smaller. You can try out the base Zephyr model using this space.\nWe download zephyr-7B-beta using the following command (this can take a while since the model is about 15 GB):\ngit lfs install\ngit clone https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\nGPTQ also requires a calibration dataset, which is used to measure the impact of the quantization process by comparing the outputs of the base model and its quantized version. We will use the wikitext dataset and directly download the test file as follows:\nwget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\nOnce it’s done, we can leverage the convert.py script provided by the ExLlamaV2 library. We’re mostly concerned with four arguments:\n\n-i: Path of the base model to convert in HF format (FP16).\n-o: Path of the working directory with temporary files and final output.\n-c: Path of the calibration dataset (in Parquet format).\n-b: Target average number of bits per weight (bpw). For example, 4.0 bpw will give store weights in 4-bit precision.\n\nThe complete list of arguments is available on this page. Let’s start the quantization process using the convert.py script with the following arguments:\nmkdir quant\npython python exllamav2/convert.py \\\n    -i base_model \\\n    -o quant \\\n    -c wikitext-test.parquet \\\n    -b 5.0\nNote that you will need a GPU to quantize this model. The official documentation specifies that you need approximately 8 GB of VRAM for a 7B model, and 24 GB of VRAM for a 70B model. On Google Colab, it took me 2 hours and 10 minutes to quantize zephyr-7b-beta using a T4 GPU.\nUnder the hood, ExLlamaV2 leverages the GPTQ algorithm to lower the precision of the weights while minimizing the impact on the ouput. You can find more details about the GPTQ algorithm in this article.\nSo why are we using the “EXL2” format instead of the regular GPTQ format? EXL2 comes with a few new features:\n\nIt supports different levels of quantization: it’s not restricted to 4-bit precision and can handle 2, 3, 4, 5, 6, and 8-bit quantization.\nIt can mix different precisions within a model and within each layer to preserve the most important weights and layers with more bits.\n\nExLlamaV2 uses this additional flexibility during quantization. It tries different quantization parameters and measures the error they introduce. On top of trying to minimize the error, ExLlamaV2 also has to achieve the target average number of bits per weight given as an argument. Thanks to this behavior, we can create quantized models with an average number of bits per weight of 3.5 or 4.5 for example.\nThe benchmark of different parameters it creates is saved in the measurement.json file. The following JSON shows the measurement for one layer:\n\"key\": \"model.layers.0.self_attn.q_proj\",\n\"numel\": 16777216,\n\"options\": [\n    {\n        \"desc\": \"0.05:3b/0.95:2b 32g s4\",\n        \"bpw\": 2.1878662109375,\n        \"total_bits\": 36706304.0,\n        \"err\": 0.011161142960190773,\n        \"qparams\": {\n            \"group_size\": 32,\n            \"bits\": [\n                3,\n                2\n            ],\n            \"bits_prop\": [\n                0.05,\n                0.95\n            ],\n            \"scale_bits\": 4\n        }\n    },\nIn this trial, ExLlamaV2 used 5% of 3-bit and 95% of 2-bit precision for an average value of 2.188 bpw and a group size of 32. This introduced a noticeable error that is taken into account to select the best parameters."
  },
  {
    "objectID": "posts/ExLlamaV2_The_Fastest_Library_to_Run LLMs.html#running-exllamav2-for-inference",
    "href": "posts/ExLlamaV2_The_Fastest_Library_to_Run LLMs.html#running-exllamav2-for-inference",
    "title": "ExLlamaV2: The Fastest Library to Run LLMs",
    "section": "🦙 Running ExLlamaV2 for Inference",
    "text": "🦙 Running ExLlamaV2 for Inference\nNow that our model is quantized, we want to run it to see how it performs. Before that, we need to copy essential config files from the base_model directory to the new quant directory. Basically, we want every file that is not hidden (.*) or a safetensors file. Additionally, we don’t need the out_tensor directory that was created by ExLlamaV2 during quantization.\nIn bash, you can implement this as follows:\n!rm -rf quant/out_tensor\n!rsync -av --exclude='*.safetensors' --exclude='.*' ./base_model/ ./quant/\nOur EXL2 model is ready and we have several options to run it. The most straightforward method consists of using the test_inference.py script in the ExLlamaV2 repo (note that I don’t use a chat template here):\npython exllamav2/test_inference.py -m quant/ -p \"I have a dream\"\nThe generation is very fast (56.44 tokens/second on a T4 GPU), even compared to other quantization techniques and tools like GGUF/llama.cpp or GPTQ. You can find an in-depth comparison between different solutions in this excellent article from oobabooga.\nIn my case, the LLM returned the following output:\n-- Model: quant/\n -- Options: ['rope_scale 1.0', 'rope_alpha 1.0']\n -- Loading model...\n -- Loading tokenizer...\n -- Warmup...\n -- Generating...\n\nI have a dream. &lt;|user|&gt;\nWow, that's an amazing speech! Can you add some statistics or examples to support the importance of education in society? It would make it even more persuasive and impactful. Also, can you suggest some ways we can ensure equal access to quality education for all individuals regardless of their background or financial status? Let's make this speech truly unforgettable!\n\nAbsolutely! Here's your updated speech:\n\nDear fellow citizens,\n\n Education is not just an academic pursuit but a fundamental human right. It empowers people, opens doors\n\n -- Response generated in 3.40 seconds, 128 tokens, 37.66 tokens/second (includes prompt eval.)\nAlternatively, you can use a chat version with the chat.py script for more flexibility:\npython exllamav2/examples/chat.py -m quant -mode llama\nIf you’re planning to use an EXL2 model more regularly, ExLlamaV2 has been integrated into several backends like oobabooga’s text generation web UI. Note that it requires FlashAttention 2 to run as efficiently as possible, which requires CUDA 12.1 on Windows at the moment (something you can configure during the installation process).\nNow that we tested the model, we’re ready to upload it to the Hugging Face Hub. You can change the name of your repo in the following code snippet and simply run it.\nfrom huggingface_hub import notebook_login\nfrom huggingface_hub import HfApi\n\nnotebook_login()\napi = HfApi()\napi.create_repo(\n    repo_id=f\"mlabonne/zephyr-7b-beta-5.0bpw-exl2\",\n    repo_type=\"model\"\n)\napi.upload_folder(\n    repo_id=f\"mlabonne/zephyr-7b-beta-5.0bpw-exl2\",\n    folder_path=\"quant\",\n)\nGreat, the model can be found on the Hugging Face Hub. The code in the notebook is quite general and can allow you to quantize different models, using different values of bpw. This is ideal for creating models dedicated to your hardware."
  },
  {
    "objectID": "posts/ExLlamaV2_The_Fastest_Library_to_Run LLMs.html#conclusion",
    "href": "posts/ExLlamaV2_The_Fastest_Library_to_Run LLMs.html#conclusion",
    "title": "ExLlamaV2: The Fastest Library to Run LLMs",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we presented ExLlamaV2, a powerful library to quantize LLMs. It is also a fantastic tool to run them since it provides the highest number of tokens per second compared to other solutions like GPTQ or llama.cpp. We applied it to the zephyr-7B-beta model to create a 5.0 bpw version of it, using the new EXL2 format. After quantization, we tested our model to see how it performs. Finally, it was then uploaded to the Hugging Face Hub and can be found here."
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html",
    "href": "posts/4_bit_Quantization_with_GPTQ.html",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "",
    "text": "Recent advancements in weight quantization allow us to run massive large language models on consumer hardware, like a LLaMA-30B model on an RTX 3090 GPU. This is possible thanks to novel 4-bit quantization techniques with minimal performance degradation, like GPTQ, GGML, and NF4.\nIn the previous article, we introduced naïve 8-bit quantization techniques and the excellent LLM.int8(). In this article, we will explore the popular GPTQ algorithm to understand how it works and implement it using the AutoGPTQ library.\nYou can find the code on Google Colab and GitHub.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "2. Quantization with GPTQ"
    ]
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#optimal-brain-quantization",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#optimal-brain-quantization",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "🧠 Optimal Brain Quantization",
    "text": "🧠 Optimal Brain Quantization\nLet’s start by introducing the problem we’re trying to solve. For every layer \\ell in the network, we want to find a quantized version \\widehat{\\mathbf{W}}_\\ell of the original weights \\mathbf{W}_\\ell. This is called the layer-wise compression problem. More specifically, to minimize performance degradation, we want the outputs (\\mathbf{\\widehat{W}_\\ell X_\\ell}) of these new weights to be as close as possible to the original ones (\\mathbf{W_\\ell X_\\ell}). In other words, we want to find:\n\\arg \\min_{\\mathbf{\\widehat{W}}_\\ell} \\parallel\\mathbf{W_\\ell X_\\ell} - \\mathbf{\\widehat{W}_\\ell X_\\ell}\\parallel_2^2.\nDifferent approaches have been proposed to solve this problem, but we’re interested in the Optimal Brain Quantizer (OBQ) framework here.\nThis method is inspired by a pruning technique to carefully remove weights from a fully trained dense neural network (Optimal Brain Surgeon). It uses an approximation technique and provides explicit formulas for the best single weight w_q to remove and optimal update \\delta_F to adjust the set of remaining non-quantized weights F to make up for the removal:\n\\begin{align*}\nw_q &= \\arg\\min_{w_q} \\frac{(\\text{quant}(w_q) - w_q)^2}{[\\mathbf{H}_F^{-1}]_{qq}},\\\\ \\quad \\delta_F &= -\\frac{w_q - \\text{quant}(w_q)}{[\\mathbf{H}_F^{-1}]_{qq}} \\cdot (\\mathbf{H}_F^{-1})_{:,q}.\n\\end{align*}\nwhere \\text{quant}(w) is the weight rounding given by the quantization and \\mathbf{H}_F is the Hessian.\nUsing OBQ, we can quantize the easiest weight first and then adjust all remaining non-quantized weights to compensate for this precision loss. Then we pick the next weight to quantize, and so on.\nA potential issue with this approach is when there are outlier weights, which can result in high quantization error. Usually, these outliers would be quantized last, when there are few non-quantized weights left that could be adjusted to compensate for the large error. This effect can worsen when some weights are pushed further outside the grid by intermediate updates. A simple heuristic is applied to prevent this: outliers are quantized as soon as they appear.\nThis process could be computationally heavy, especially for LLMs. To deal with this, the OBQ method uses a trick that avoids redoing the entire computation each time a weight is simplified. After quantizing a weight, it adjusts the matrix used in calculations (the Hessian) by removing the row and column associated with that weight (using Gaussian elimination):\n\n\\mathbf{H}^{-1}_{-q} = \\left( \\mathbf{H}^{-1} - \\frac{1}{[\\mathbf{H}^{-1}]_{qq}} \\mathbf{H}^{-1}_{:,q} \\mathbf{H}^{-1}_{q,:} \\right)_{-p}.\n\nThe method also employs vectorization to process multiple rows of the weight matrix at once. Despite its efficiency, the OBQ’s computation time increases significantly as the size of the weight matrix increases. This cubic growth makes it difficult to use OBQ on very large models with billions of parameters.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "2. Quantization with GPTQ"
    ]
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#the-gptq-algorithm",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#the-gptq-algorithm",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "🧮 The GPTQ Algorithm",
    "text": "🧮 The GPTQ Algorithm\nIntroduced by Frantar et al. (2023), the GPTQ algorithm takes inspiration from the OBQ method, but with significant improvements to scale it for (very) large language models.\n\nStep 1: Arbitrary Order Insight\nThe OBQ method selects weights (parameters in a model) for quantization in a certain order, determined by which will add the least additional error. However, GPTQ observes that for large models, quantizing weights in any fixed order can perform just as well. This is because even though some weights might introduce more error individually, they are quantized later in the process when there are few other weights left that could increase the error. So the order doesn’t matter as much as we thought.\nBased on this insight, GPTQ aims to quantize all weights in the same order for all rows of a matrix. This makes the process faster because certain computations have to be done only once for each column, rather than once for each weight.\n\n\n\n\n\nStep 2: Lazy Batch-Updates\nThis scheme won’t be fast because it requires updating a huge matrix with very few computations for each entry. This type of operation can’t utilize the full compute capabilities of GPUs and will be slowed down by memory limitations (memory throughput bottleneck).\nTo resolve this, GPTQ introduces “lazy batch” updates. It turns out that the final rounding decisions for a given column are only affected by updates performed on that column, not on later columns. Therefore, GPTQ can apply the algorithm to a batch of columns at a time (like 128 columns), updating only those columns and a corresponding block of the matrix. After a block is fully processed, the algorithm performs global updates on the entire matrix.\n\\begin{align*}\n\\mathbf{\\delta}_F &= -(\\mathbf{w}_Q - \\text{quant}(\\mathbf{w}_Q))([\\mathbf{H}_F^{-1}]_{QQ})^{-1} (\\mathbf{H}_F^{-1})_{:,Q}, \\\\\n\\mathbf{H}^{-1}_{-Q} &= \\left(\\mathbf{H}^{-1} - \\mathbf{H}^{-1}_{:,Q}([\\mathbf{H}_F^{-1}]_{QQ})^{-1}\\mathbf{H}^{-1}_{Q,:}\\right)_{-Q}.\n\\end{align*}\n\n\nStep 3: Cholesky Reformulation\nHowever, there’s one more issue to address. When the algorithm scales up to very large models, numerical inaccuracies can become a problem. Specifically, repeated applications of a certain operation can accumulate numerical errors.\nTo tackle this, GPTQ uses a Cholesky decomposition, a numerically stable method for solving certain mathematical problems. It involves precomputing some required information from the matrix using the Cholesky method. This approach, combined with a slight “dampening” (adding a small constant to diagonal elements of the matrix), helps the algorithm to avoid numerical issues.\nThe full algorithm can be summarized in a few steps:\n\nThe GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse (a matrix that helps decide how to adjust the weights)\nIt then runs in loops, handling batches of columns at a time.\nFor each column in a batch, it quantizes the weights, calculates the error, and updates the weights in the block accordingly.\nAfter processing the batch, it updates all remaining weights based on the block’s errors.\n\nThe GPTQ algorithm was tested on various language generation tasks. It was compared with other quantization methods, like rounding all weights to the nearest quantized value (RTN). GPTQ was used with the BLOOM (176B parameters) and OPT (175B parameters) model families, and models were quantized using a single NVIDIA A100 GPU.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "2. Quantization with GPTQ"
    ]
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#quantize-an-llm-with-autogptq",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#quantize-an-llm-with-autogptq",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "💻 Quantize an LLM with AutoGPTQ",
    "text": "💻 Quantize an LLM with AutoGPTQ\nGPTQ has been very popular to create models in 4-bit precision that can efficiently run on GPUs. You can find many examples on the Hugging Face Hub, especially from TheBloke. If you’re looking for an approach that is more CPU-friendly, GGML is currently your best option. Finally, the transformers library with bitsandbytes allows you to quantize a model when it’s loaded using the load_in_4bit=true argument, which requires downloading full models and storing them in your RAM.\nLet’s implement the GPTQ algorithm using the AutoGPTQ library and quantize a GPT-2 model. This requires a GPU, but a free T4 on Google Colab will do. We start by loading the libraries and defining the model we want to quantize (in this case, GPT-2).\n\n!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers\n\n\nimport random\n\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom datasets import load_dataset\nimport torch\nfrom transformers import AutoTokenizer\n\n\n# Define base model and output directory\nmodel_id = \"gpt2\"\nout_dir = model_id + \"-GPTQ\"\n\nWe now want to load the model and the tokenizer. The tokenizer is loaded using the classic AutoTokenizer class from the transformers library. On the other hand, we need to pass a specific configuration (BaseQuantizeConfig) to load the model.\nIn this configuration, we can specify the number of bits to quantize (here, bits=4) and the group size (size of the lazy batch). Note that this group size is optional: we could also use one set of parameters for the entire weight matrix. In practice, these groups generally improve the quality of the quantization at a very low cost (especially with group_size=1024). The damp_percent value is here to help the Cholesky reformulation and should not be changed.\nFinally, the desc_act (also called act order) is a tricky parameter. It allows you to process rows based on decreasing activation, meaning the most important or impactful rows (determined by sampled inputs and outputs) are processed first. This method aims to place most of the quantization error (inevitably introduced during quantization) on less significant weights. This approach improves the overall accuracy of the quantization process by ensuring the most significant weights are processed with greater precision. However, when used alongside group size, desc_act can lead to performance slowdowns due to the need to frequently reload quantization parameters. For this reason, we won’t use it here (it will probably be fixed in the future, however).\n\n# Load quantize config, model and tokenizer\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False,\n)\nmodel = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nThe quantization process relies heavily on samples to evaluate and enhance the quality of the quantization. They provide a means of comparison between the outputs produced by the origina and the newly quantized model. The larger the number of samples provided, the greater the potential for more accurate and effective comparisons, leading to improved quantization quality.\nIn the context of this article, we utilize the C4 (Colossal Clean Crawled Corpus) dataset to generate our samples. The C4 dataset is a large-scale, multilingual collection of web text gathered from the Common Crawl project. This expansive dataset has been cleaned and prepared specifically for training large-scale language models, making it a great resource for tasks such as this. The WikiText dataset is another popular option.\nIn the following code block, we load 1024 samples from the C4 dataset, tokenize them, and format them.\n\n# Load data and tokenize examples\nn_samples = 1024\ndata = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\ntokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n\n# Format tokenized examples\nexamples_ids = []\nfor _ in range(n_samples):\n    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n    j = i + tokenizer.model_max_length\n    input_ids = tokenized_data.input_ids[:, i:j]\n    attention_mask = torch.ones_like(input_ids)\n    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n\nWARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/allenai___json/allenai--c4-6e494e9c0ee1404e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\nToken indices sequence length is longer than the specified maximum sequence length for this model (2441065 &gt; 1024). Running this sequence through the model will result in indexing errors\n\n\nNow that dataset is ready, we can start the quantization process with a batch size of 1. Optionally, we also use OpenAI Triton, a CUDA alternative, to communicate with the GPU. Once this is done, we save the tokenizer and the model in a safetensors format.\n\n%%time\n\n# Quantize with GPTQ\nmodel.quantize(\n    examples_ids,\n    batch_size=1,\n    use_triton=True,\n)\n\n# Save model and tokenizer\nmodel.save_quantized(out_dir, use_safetensors=True)\ntokenizer.save_pretrained(out_dir)\n\nCPU times: user 4min 35s, sys: 3.49 s, total: 4min 39s\nWall time: 5min 8s\n\n\n('gpt2-GPTQ/tokenizer_config.json',\n 'gpt2-GPTQ/special_tokens_map.json',\n 'gpt2-GPTQ/vocab.json',\n 'gpt2-GPTQ/merges.txt',\n 'gpt2-GPTQ/added_tokens.json',\n 'gpt2-GPTQ/tokenizer.json')\n\n\nAs per usual, the model and tokenizer can then be loaded from the output directory using the AutoGPTQForCausalLM and AutoTokenizer classes.\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Reload model and tokenizer\nmodel = AutoGPTQForCausalLM.from_quantized(\n    out_dir,\n    device=device,\n    use_triton=True,\n    use_safetensors=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(out_dir)\n\nWARNING:accelerate.utils.modeling:The safetensors archive passed at gpt2-GPTQ/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\nWARNING:auto_gptq.modeling._base:GPT2GPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\nWARNING:auto_gptq.modeling._base:GPT2GPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n\n\nLet’s check that the model is working correctly. The AutoGPTQ model (mostly) works as a normal transformers model, which makes it compatible with inference pipelines, as shown in the following example:\n\nfrom transformers import pipeline\n\ngenerator = pipeline('text-generation', model=model, tokenizer=tokenizer)\nresult = generator(\"I have a dream\", do_sample=True, max_length=50)[0]['generated_text']\nprint(result)\n\nThe model 'GPT2GPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nI have a dream,\" she told CNN last week. \"I have this dream of helping my mother find her own. But, to tell that for the first time, now that I'm seeing my mother now, just knowing how wonderful it is that\n\n\nWe managed to get a convincing completion from our quantized GPT-2 model. A more in-depth evaluation would require measuring the perplexity of the quantized model versus the original one. However, we will leave it out of the scope of this article.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "2. Quantization with GPTQ"
    ]
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#conclusion",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#conclusion",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we introduced the GPTQ algorithm, a state-of-the-art quantization technique to run LLMs on consumer-grade hardware. We showed how it addresses the layer-wise compression problem, based on an improved OBS technique with arbitrary order insight, lazy batch updates, and Cholesky reformulation. This novel approach significantly reduces memory and computation requirements, making LLMs accessible to a broader audience.\nIn addition, we quantized our own LLM model on a free T4 GPU and ran it to generate text. You can push your own version of a GPTQ 4-bit quantized model on the Hugging Face Hub. As mentioned in the introduction, GPTQ is not the only 4-bit quantization algorithm: GGML and NF4 are excellent alternatives with slightly different scopes. I encourage you to learn more about them and give them a shot!\nIf you’re interested in more technical content around LLMs, follow me on Twitter @maximelabonne.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "2. Quantization with GPTQ"
    ]
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#references",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#references",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "References",
    "text": "References\n\nB. Hassibi, D. G. Stork and G. J. Wolff, “Optimal Brain Surgeon and general network pruning,” IEEE International Conference on Neural Networks, San Francisco, CA, USA, 1993, pp. 293-299 vol.1, doi: 10.1109/ICNN.1993.298572.\nElias Frantar, Sidak Pal Singh, & Dan Alistarh. (2023). Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, & Dan Alistarh. (2023). GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, & Peter J. Liu. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "⚡ **LLM Quantization**",
      "2. Quantization with GPTQ"
    ]
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html",
    "title": "A Beginner’s Guide to LLM Fine-Tuning",
    "section": "",
    "text": "The growing interest in Large Language Models (LLMs) has led to a surge in tools and wrappers designed to streamline their training process.\nPopular options include FastChat from LMSYS (used to train Vicuna) and Hugging Face’s transformers/trl libraries (used in my previous article). In addition, each big LLM project, like WizardLM, tends to have its own training script, inspired by the original Alpaca implementation.\nIn this article, we will use Axolotl, a tool created by the OpenAccess AI Collective. We will use it to fine-tune a Code Llama 7b model on an evol-instruct dataset comprised of 1,000 samples of Python code.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "2. Fine-tune Llama 2 in Axolotl"
    ]
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#why-axolotl",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#why-axolotl",
    "title": "A Beginner’s Guide to LLM Fine-Tuning",
    "section": "🤔 Why Axolotl?",
    "text": "🤔 Why Axolotl?\nThe main appeal of Axolotl is that it provides a one-stop solution, which includes numerous features, model architectures, and an active community. Here’s a quick list of my favorite things about it:\n\nConfiguration: All parameters used to train an LLM are neatly stored in a yaml config file. This makes it convenient for sharing and reproducing models. You can see an example for Llama 2 here.\nDataset Flexibility: Axolotl allows the specification of multiple datasets with varied prompt formats such as alpaca ({\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}), sharegpt:chat ({\"conversations\": [{\"from\": \"...\", \"value\": \"...\"}]}), and raw completion ({\"text\": \"...\"}). Combining datasets is seamless, and the hassle of unifying the prompt format is eliminated.\nFeatures: Axolotl is packed with SOTA techniques such as FSDP, deepspeed, LoRA, QLoRA, ReLoRA, sample packing, GPTQ, FlashAttention, xformers, and rope scaling.\nUtilities: There are numerous user-friendly utilities integrated, including the addition or alteration of special tokens, or a custom wandb configuration.\n\nSome well-known models trained using this tool are Manticore-13b from the OpenAccess AI Collective and Samantha-1.11-70b from Eric Hartford. Like other wrappers, it is built on top of the transformers library and uses many of its features.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "2. Fine-tune Llama 2 in Axolotl"
    ]
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#create-your-own-config-file",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#create-your-own-config-file",
    "title": "A Beginner’s Guide to LLM Fine-Tuning",
    "section": "⚙️ Create your own config file",
    "text": "⚙️ Create your own config file\nBefore anything, we need a configuration file. You can reuse an existing configuration from the examples folder. In our case, we will tweak the QLoRA config for Llama 2 to create our own Code Llama model. The model will be trained on a subset of 1,000 Python samples from the nickrosh/Evol-Instruct-Code-80k-v1 dataset.\nFirst, we must change the base_model and base_model_config fields to “codellama/CodeLlama-7b-hf”. To push our trained adapter to the Hugging Face Hub, let’s add a new field hub_model_id, which corresponds to the name of our model, “EvolCodeLlama-7b”. Now, we have to update the dataset to mlabonne/Evol-Instruct-Python-1k and set type to “alpaca”.\nThere’s no sample bigger than 2048 tokens in this dataset, so we can reduce the sequence_len to “2048” and save some VRAM. Talking about VRAM, we’re going to use a micro_batch_size of 10 and a gradient_accumulation_steps of 1 to maximize its use. In practice, you try different values until you use &gt;95% of the available VRAM.\nFor convenience, I’m going to add the name “axolotl” to the wandb_project field so it’s easier to track on my account. I’m also setting the warmup_steps to “100” (personal preference) and the eval_steps to 0.01 so we’ll end up with 100 evaluations.\nHere’s how the final config file should look:\nbase_model: codellama/CodeLlama-7b-hf\nbase_model_config: codellama/CodeLlama-7b-hf\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\nis_llama_derived_model: true\nhub_model_id: EvolCodeLlama-7b\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n    - path: mlabonne/Evol-Instruct-Python-1k\n    type: alpaca\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.02\noutput_dir: ./qlora-out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 2048\nsample_packing: true\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: axolotl\nwandb_entity:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps: 1\nmicro_batch_size: 10\nnum_epochs: 3\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: true\nfp16: false\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 100\neval_steps: 0.01\nsave_strategy: epoch\nsave_steps:\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n    bos_token: \"&lt;s&gt;\"\n    eos_token: \"&lt;/s&gt;\"\n    unk_token: \"&lt;unk&gt;\"\nYou can also find this config file here as a GitHub gist.\nBefore we start training our model, I want to introduce a few parameters that are important to understand:\n\nQLoRA: We’re using QLoRA for fine-tuning, which is why we’re loading the base model in 4-bit precision (NF4 format). You can check this article from Benjamin Marie to know more about QLoRA.\nGradient checkpointing: It lowers the VRAM requirements by removing some activations that are re-computed on demand during the backward pass. It also slows down training by about 20%, according to Hugging Face’s documentation.\nFlashAttention: This implements the FlashAttentionmechanism, which improves the speed and memory efficiency of our model thanks to a clever fusion of GPU operations (learn more about it in this article from Aleksa Gordiç).\nSample packing: Smart way of creating batches with as little padding as possible, by reorganizing the order of the samples (bin packing problem). As a result, we need fewer batches to train the model on the same dataset. It was inspired by the Multipack Sampler (see my note) and Krell et al.\n\nYou can find FlashAttention in some other tools, but sample packing is relatively new. As far as I know, OpenChatwas the first project to use sample packing during fine-tuning. Thanks to Axolotl, we’ll use these techniques for free.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "2. Fine-tune Llama 2 in Axolotl"
    ]
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#fine-tune-code-llama",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#fine-tune-code-llama",
    "title": "A Beginner’s Guide to LLM Fine-Tuning",
    "section": "🦙 Fine-tune Code Llama",
    "text": "🦙 Fine-tune Code Llama\nHaving the config file ready, it’s time to get our hands dirty with the actual fine-tuning. You might consider running the training on a Colab notebook. However, for those without access to a high-performance GPU, a more cost-effective solution consists of renting cloud-based GPU services, like AWS, Lambda Labs, Vast.ai, Banana, or RunPod.\nPersonally, I use RunPod, which is a popular option in the fine-tuning community. It’s not the cheapest service but it hits a good tradeoff with a clean UI. You can easily replicate the following steps using your favorite service.\nWhen your RunPod account is set up, go to Manage &gt; Templates and click on “New Template”. Here is a simple template:\n\n\n\nLet’s review the different fields and their corresponding values:\n\nTemplate Name: Axolotl (you can choose whatever you want)\nContainer Image: winglian/axolotl-runpod:main-py3.10-cu118-2.0.1\nContainer Disk: 100 GB\nVolume Disk: 0 GB\nVolume Mount Path: /workspace\n\nIn addition, there are two handy environment variables can include:\n\nHUGGING_FACE_HUB_TOKEN: you can find your token on this page (requires an account)\nWANDB_API_KEY: you can find your key on this page (requires an account)\n\nAlternatively, you can simply log in the terminal later (using huggingface-cli login and wandb login). Once you’re set-up, go to Community Cloud and deploy an RTX 3090. Here you can search for the name of your template and select it as follows:\n\n\n\nYou can click on “Continue” and RunPod will deploy your template. You can see the installation in your pod’s logs (Manage &gt; Pods). When the option becomes available, click on “Connect”. Here, click on “tart Web Terminal” and then “Connect to Web Terminal”. You are now connected to your pod!\nThe following steps are the same no matter what service you choose:\n\nWe install Axolotl and the PEFT library as follows:\n\ngit clone https://github.com/OpenAccess-AI-Collective/axolotl\ncd axolotl\n\npip3 install -e .[flash-attn]\npip3 install -U git+https://github.com/huggingface/peft.git\n\nDownload the config file we created:\n\nwget https://gist.githubusercontent.com/mlabonne/8055f6335e2b85f082c8c75561321a66/raw/93915a9563fcfff8df9a81fc0cdbf63894465922/EvolCodeLlama-7b.yaml\n\nYou can now start fine-tuning the model with the following command:\n\naccelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml\nIf everything is configured correctly, you should be able to train the model in a little more than one hour (it took me 1h 11m 44s). If you check the GPU memory used, you’ll see almost 100% with this config, which means we’re optimizing it pretty nicely. If you’re using a GPU with more VRAM (like an A100), you can increase the micro-batch size to make sure you’re fully using it.\nIn the meantime, feel free to close the web terminal and check your loss on Weights & Biases. We’re using tmux so the training won’t stop if you close the terminal. Here are my loss curves:\n\n\n\nWe see a steady improvement in the eval loss, which is a good sign. However, you can also spot drops in the eval loss that are not correlated with a decrease in the quality of the outputs. The best way to evaluate your model is simply by using it: you can run it in the terminal with the command accelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml –inference –lora_model_dir=“./qlora-out”.\nThe QLoRA adapter should already be uploaded to the Hugging Face Hub. However, you can also merge the base Code Llama model with this adapter and push the merged model there by following these steps:\n\nDownload this script:\n\nwget https://gist.githubusercontent.com/mlabonne/a3542b0519708b8871d0703c938bba9f/raw/60abc5afc07f9d843bc23d56f4e0b7ab072c4a62/merge_peft.py\n\nExecute it with this command:\n\npython merge_peft.py --base_model=codellama/CodeLlama-7b-hf --peft_model=./qlora-out --hub_id=EvolCodeLlama-7b\nCongratulations, you should have your own EvolCodeLlama-7b on the Hugging Face Hub at this point! For reference, you can access my own model trained with this process here: mlabonne/EvolCodeLlama-7b\nConsidering that our EvolCodeLlama-7b is a code LLM, it would be interesting to compare its performance with other models on standard benchmarks, such as HumanEval and MBPP. For reference, you can find a leaderboard at the following address: Multilingual Code Evals.\nIf you’re happy with this model, you can quantize it with GGML for local inference with this free Google Colab notebook. You can also fine-tune bigger models (e.g., 70b parameters) thanks to deepspeed, which only requires an additional config file.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "2. Fine-tune Llama 2 in Axolotl"
    ]
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#conclusion",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#conclusion",
    "title": "A Beginner’s Guide to LLM Fine-Tuning",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we’ve covered the essentials of how to efficiently fine-tune LLMs. We customized parameters to train on our Code Llama model on a small Python dataset. Finally, we merged the weights and uploaded the result on Hugging Face.\nI hope you found this guide useful. I recommend using Axolotl with a cloud-based GPU service to get some experience and upload a few models on Hugging Face. Build your own datasets, play with the parameters, and break stuff along the way. Like with every wrapper, don’t hesitate to check the source code to get a good intuition of what it’s actually doing. It will massively help in the long run.\nThanks to the OpenAccess AI Collective and all the contributors!",
    "crumbs": [
      "{{< fa chalkboard >}} LLM",
      "🗣️ **Large Language Models**",
      "2. Fine-tune Llama 2 in Axolotl"
    ]
  },
  {
    "objectID": "demo.html",
    "href": "demo.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  }
]